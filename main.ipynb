{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:45.537113Z",
     "start_time": "2023-06-20T12:32:39.998206100Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from scipy.optimize import minimize, least_squares\n",
    "from torch import float64\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from linreg import Methods, LearningRateScheduling\n",
    "\n",
    "\n",
    "def optimizer_handler(method, params, lr, beta_1=0.9, beta_2=0.999, factor=10):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return torch.optim.SGD(params, lr)\n",
    "        case Methods.Momentum:\n",
    "            return torch.optim.SGD(params, lr, beta_1)\n",
    "        case Methods.AdaGrad:\n",
    "            return torch.optim.Adagrad(params, lr * factor)\n",
    "        case Methods.RMSprop:\n",
    "            return torch.optim.RMSprop(params, lr, alpha=beta_2)\n",
    "        case Methods.Adam:\n",
    "            return torch.optim.Adam(params, lr, betas=(beta_1, beta_2))\n",
    "        case Methods.Nesterov:\n",
    "            return torch.optim.SGD(params, lr, nesterov=True, momentum=beta_1)\n",
    "\n",
    "\n",
    "def lr_scheduler_handler(optimizer, lrs, lr, epoch_size=30):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda *_: 1)\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_size, gamma=0.75)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "\n",
    "class TorchLinearRegression:\n",
    "    def __init__(self, T, X, Y, W=None):\n",
    "        self.T_funcs = T\n",
    "        self.T = torch.tensor([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))], dtype=float64).reshape(\n",
    "            len(X), len(T))\n",
    "        self.X = torch.tensor(X, dtype=float64)\n",
    "        self.Y = torch.tensor(Y, dtype=float64)\n",
    "        self.loss_f = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.refresh(W)\n",
    "\n",
    "    def optimize(self, method=Methods.Classic, lr=0.01, lrs=LearningRateScheduling.Classic, max_steps=1500):\n",
    "        optimizer = optimizer_handler(method, [self.W], lr)\n",
    "        scheduler = lr_scheduler_handler(optimizer, lrs, lr=lr)\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            optimizer.zero_grad()\n",
    "            self.loss(self.W, is_no_grad=False)\n",
    "            optimizer.step()\n",
    "            self.W_points.append(self.W.clone().detach().numpy())\n",
    "            scheduler.step()\n",
    "\n",
    "        return self.W\n",
    "\n",
    "    def loss(self, W, is_no_grad=True):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W).reshape(len(self.T_funcs), 1)\n",
    "            W = Variable(torch.tensor(W, dtype=float64), requires_grad=True)\n",
    "        if is_no_grad:\n",
    "            with torch.no_grad():\n",
    "                loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "                return float(loss_val) * len(self.X)\n",
    "        else:\n",
    "            loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "            loss_val.backward()\n",
    "            return float(loss_val)\n",
    "\n",
    "    def refresh(self, W=None):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W, dtype=float64).reshape(len(self.T_funcs), 1)\n",
    "        if W is None:\n",
    "            W = torch.randn(len(self.T_funcs), 1, dtype=float64)\n",
    "        self.W = Variable(torch.tensor(W), requires_grad=True)\n",
    "        self.W_points = [torch.clone(self.W).detach().numpy()]\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (torch.linalg.inv(torch.t(self.T) @ self.T) @ torch.t(self.T)) @ self.Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:46.211061Z",
     "start_time": "2023-06-20T12:32:45.552736600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "from time import time\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, result, mem, time, other=None):\n",
    "        if other is None:\n",
    "            other = []\n",
    "        self.result = result\n",
    "        self.mem = mem\n",
    "        self.time = time\n",
    "        self.other = other\n",
    "\n",
    "\n",
    "def get_metrics(func):\n",
    "    start = time()\n",
    "    tracemalloc.start()\n",
    "    result = func()\n",
    "    mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end = time()\n",
    "\n",
    "    return Metrics(result, mem[1] - mem[0], end - start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:46.276511500Z",
     "start_time": "2023-06-20T12:32:46.226698300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from excel import ExcellSaver\n",
    "\n",
    "excel_saver = ExcellSaver()\n",
    "\n",
    "\n",
    "def add_metrics_row(metrics):\n",
    "    global excel_saver\n",
    "\n",
    "    row = []\n",
    "    for pair in metrics.other:\n",
    "        row.append(pair[1])\n",
    "    row.append(metrics.time)\n",
    "    row.append(metrics.mem)\n",
    "\n",
    "    excel_saver.add_row(row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:47.595216100Z",
     "start_time": "2023-06-20T12:32:46.276511500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "from linreg import gen_linear_reg, visualise_approximation, sgd_handler\n",
    "\n",
    "Output = Enum(\"Output\", [\"No\", \"Console\", \"Excel\"])\n",
    "\n",
    "\n",
    "def visualise(f, points, title, x_label=\"x\", y_label=\"y\"):\n",
    "    values = np.transpose(np.array(points))\n",
    "    X = np.linspace(min(values[0]) - 10, max(values[0]) + 10, 100)\n",
    "    Y = np.linspace(min(values[1]) - 10, max(values[1]) + 10, 100)\n",
    "    Z = [[f(np.array([X[i], Y[j]])) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 80)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.title(title)\n",
    "    plt.legend(['Route', 'Start point', 'End point'])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_errors(excepted, received, suf=\"\"):\n",
    "    print(\"Excepted \" + suf + \": \" + str(excepted))\n",
    "    print(\"Received \" + suf + \": \" + str(received))\n",
    "    print(\"Absolute error \" + suf + \": \" + str(np.linalg.norm(excepted - received)))\n",
    "    print(\"Relative error \" + suf + \": \" + str(np.linalg.norm(excepted - received) / np.linalg.norm(excepted)))\n",
    "\n",
    "\n",
    "def add_linreg_errors_info(metrics, linreg):\n",
    "    expected_linreg_loss = linreg.loss(linreg.analytical_solution())\n",
    "    linreg_loss = linreg.loss(linreg.W)\n",
    "    metrics.other.append([\"Expected Loss\", expected_linreg_loss])\n",
    "    metrics.other.append([\"Actual Loss\", linreg_loss])\n",
    "\n",
    "\n",
    "def test_sgd_variants(output=Output.No):\n",
    "    global excel_saver\n",
    "\n",
    "    if output == Output.Excel:\n",
    "        excel_saver.add_new_sheet([\"Whose\", \"Method\", \"Expected Loss\", \"Actual Loss\", \"Time\", \"Memory\"],\n",
    "                                  \"Test SGD Methods\")\n",
    "\n",
    "    count_2_arity = 1\n",
    "    count_other_arity = 0\n",
    "    left_coeffs_border = -2.\n",
    "    right_coeffs_border = 2.\n",
    "    left_x_border = -2.\n",
    "    right_x_border = 2.\n",
    "    deviation = 3.\n",
    "\n",
    "    for i in range(count_2_arity + count_other_arity):\n",
    "        arity = 2 if i < count_2_arity else random.randint(3, 8)\n",
    "        num_train_points = random.randint(50, 100)\n",
    "        start_point = np.array([float(random.randint(15, 30)) for i in range(arity)])\n",
    "        linreg = gen_linear_reg(\n",
    "            arity - 1, num_train_points,\n",
    "            left_coeffs_border, right_coeffs_border,\n",
    "            left_x_border, right_x_border,\n",
    "            deviation\n",
    "        )\n",
    "        torch_linreg = TorchLinearRegression(linreg.T_funcs, linreg.X, linreg.Y, torch.tensor(start_point))\n",
    "        self_scheduled = (Methods.AdaGrad, Methods.Adam, Methods.RMSprop)\n",
    "\n",
    "        for method in Methods:\n",
    "            lr = 0.01\n",
    "            if method in self_scheduled:\n",
    "                lr = 0.1\n",
    "                linreg.refresh(start_point)\n",
    "                our_metrics = get_metrics(lambda: sgd_handler(linreg, lambda *_: lr, method, store_points=True))\n",
    "                title = 'Our ' + method.name\n",
    "                if len(linreg.T_funcs) == 2:\n",
    "                    visualise(linreg.loss, linreg.W_points, title)\n",
    "                visualise_approximation(linreg, title)\n",
    "\n",
    "                title = 'Torch ' + method.name\n",
    "                torch_linreg.refresh(torch.tensor(start_point))\n",
    "                torch_metrics = get_metrics(lambda: torch_linreg.optimize(method, lr=lr))\n",
    "                if len(torch_linreg.T_funcs) == 2:\n",
    "                    visualise(torch_linreg.loss, torch_linreg.W_points, title)\n",
    "                visualise_approximation(torch_linreg, title)\n",
    "\n",
    "                match output:\n",
    "                    case Output.Excel:\n",
    "                        our_metrics.other.append([\"Whose\", \"Our\"])\n",
    "                        our_metrics.other.append([\"Method\", method.name.replace(\"Methods.\", \"\")])\n",
    "                        add_linreg_errors_info(our_metrics, linreg)\n",
    "                        add_metrics_row(our_metrics)\n",
    "\n",
    "                        torch_metrics.other.append([\"Whose\", \"Torch\"])\n",
    "                        torch_metrics.other.append([\"Method\", method.name.replace(\"Methods.\", \"\")])\n",
    "                        add_linreg_errors_info(torch_metrics, torch_linreg)\n",
    "                        add_metrics_row(torch_metrics)\n",
    "                    case Output.Console:\n",
    "                        pass\n",
    "\n",
    "\n",
    "# test_sgd_variants(Output.Excel)\n",
    "# excel_saver.create_excel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:47.626467700Z",
     "start_time": "2023-06-20T12:32:47.610845900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def derivative(f, x, i, delt=0.0001):\n",
    "    x_1 = np.copy(x)\n",
    "    x_2 = np.copy(x)\n",
    "    x_1[i] += delt\n",
    "    x_2[i] -= delt\n",
    "    y_1 = f(x_1)\n",
    "    y_2 = f(x_2)\n",
    "    return (y_1 - y_2) / (2 * delt)\n",
    "\n",
    "\n",
    "def grad(f, delt=0.01):\n",
    "    def grad_calc(x):\n",
    "        array = []\n",
    "        for i in range(len(x)):\n",
    "            array.append(derivative(f, x, i, delt))\n",
    "        return np.array(array)\n",
    "\n",
    "    return grad_calc\n",
    "\n",
    "\n",
    "def hessian(f):\n",
    "    def calc(x):\n",
    "        B = np.asarray([[0. for _ in range(len(x))] for _ in range(len(x))])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                B[i][j] = derivative(lambda x_tmp: derivative(f, x_tmp, j), x, i)\n",
    "        return B\n",
    "\n",
    "    return calc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:47.642100800Z",
     "start_time": "2023-06-20T12:32:47.626467700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "Algorithms = Enum('Methods', ['Newton', 'DogLeg', 'BFGS', 'LBFGS'])\n",
    "\n",
    "\n",
    "def optimize_handler(fun, x0, algorithm=Algorithms.Newton):\n",
    "    match algorithm:\n",
    "        case Algorithms.Newton:\n",
    "            return least_squares(fun, x0)\n",
    "        case Algorithms.DogLeg:\n",
    "            return minimize(fun, x0, method='dogleg', jac=grad(fun), hess=hessian(fun))\n",
    "        case Algorithms.BFGS:\n",
    "            return minimize(fun, x0, method='BFGS')\n",
    "        case Algorithms.LBFGS:\n",
    "            return minimize(fun, x0, method='L-BFGS-B')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:47.657724300Z",
     "start_time": "2023-06-20T12:32:47.642100800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#gen random excepted coeffs\n",
    "def gen_excepted(lb=-10, rb=10, min_size=2, max_size=10):\n",
    "    return np.random.uniform(lb, rb, size=(np.random.randint(min_size, max_size)))\n",
    "\n",
    "\n",
    "#gen parameters with poly functions of points\n",
    "def gen_parameters(excepted, N=100, deviation=0.01, is_random_N=False, lb_N=50, rb_N=200):\n",
    "    M = len(excepted)\n",
    "    if is_random_N:\n",
    "        N = np.random.randint(lb_N, rb_N)\n",
    "\n",
    "    noise = torch.randn(N, 1, dtype=float64) * deviation\n",
    "    powers = [(M - 1 - i) for i in range(M)]\n",
    "    Funcs = np.array([lambda x, i=i: (x ** powers[i]) for i in range(M)])\n",
    "    X = torch.randn(N, 1, dtype=float64)\n",
    "    Y = sum([float(excepted[i]) * Funcs[i](X) for i in range(M)]) + noise\n",
    "    T = torch.zeros(len(X), len(Funcs), dtype=float64)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Funcs)):\n",
    "            T.data[i, j] = Funcs[j](X.data[i])\n",
    "\n",
    "    return (X, Y, T, Funcs)\n",
    "\n",
    "\n",
    "#gen least squares loss function\n",
    "def gen_f(X, Y, T, Funcs):\n",
    "    def f(W):\n",
    "        W1 = None\n",
    "        if isinstance(W, torch.Tensor):\n",
    "            W1 = W\n",
    "        else:\n",
    "            W1 = np.copy(W).reshape(len(Funcs), 1)\n",
    "            W1 = torch.tensor(W1, dtype=float64)\n",
    "        model = T.mm(W1)\n",
    "        mse = nn.MSELoss()\n",
    "        res = mse(model, Y)\n",
    "        return res if isinstance(W, torch.Tensor) else res.item()\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "#gen random x0\n",
    "def gen_x0(len, lb=-20, rb=20):\n",
    "    return np.random.uniform(lb, rb, size=len)\n",
    "\n",
    "\n",
    "#numeric grad for any function\n",
    "def num_grad(f, delt=0.01):\n",
    "    return grad(f, delt=delt)\n",
    "\n",
    "\n",
    "#analityc grad for any linear function of weights with the least squares problem\n",
    "def an_grad(X, Y, T, Funcs):\n",
    "    M = len(Funcs)\n",
    "\n",
    "    def an_grad_calc(W):\n",
    "        array = []\n",
    "        components_value = ((T @ torch.tensor(W, dtype=float64).reshape(len(W), 1)) - Y)\n",
    "        for i in range(M):\n",
    "            x_fi = Funcs[i](X)\n",
    "            # torch.pow(X, (M - 1 - i))\n",
    "            array.append((x_fi.T @ components_value).item())\n",
    "        return np.array(array)\n",
    "\n",
    "    return an_grad_calc\n",
    "\n",
    "\n",
    "#torch grad for any function with the least squares problem\n",
    "def torch_grad(f):\n",
    "    def torch_grad_calc(W):\n",
    "        x = torch.tensor(W, dtype=float64, requires_grad=True).reshape(len(W), 1)\n",
    "        x.retain_grad()\n",
    "        Q = f(x)\n",
    "        Q.backward()\n",
    "        return x.grad.T.detach().numpy().reshape(len(W))\n",
    "\n",
    "    return torch_grad_calc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:47.695486300Z",
     "start_time": "2023-06-20T12:32:47.657724300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_pytorch_grad(output: Output.No):\n",
    "    count_examples = 3\n",
    "    for _ in range(count_examples):\n",
    "        X, Y, T, Funcs = gen_parameters(gen_excepted(), is_random_N=True)\n",
    "        f = gen_f(X, Y, T, Funcs)\n",
    "        x0 = gen_x0(len(Funcs))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2.a\n",
      "-------------------------\n",
      "Excepted : [ 5.69921665  7.5687605   3.79636449 -6.64472639 -3.61377124  7.69996933\n",
      "  9.037617    8.9696162   2.26304092]\n",
      "Received : [ 11.73149787 -12.72341337 -17.95880807  16.85472541  -5.33598349\n",
      " -17.63684389  -0.51492583   3.69218686   6.49710463]\n",
      "Absolute error : 47.49339169215202\n",
      "Relative error : 2.4113144568444684\n",
      "-------------------------\n",
      "\n",
      "Example 2.b\n",
      "-------------------------\n",
      "Excepted with num grad: [ 5.69921665  7.5687605   3.79636449 -6.64472639 -3.61377124  7.69996933\n",
      "  9.037617    8.9696162   2.26304092]\n",
      "Received with num grad: [ 5.69928304  7.56897431  3.79561539 -6.6463117  -3.60960013  7.70304352\n",
      "  9.02950156  8.96898161  2.26619131]\n",
      "Absolute error with num grad: 0.010303465253641715\n",
      "Relative error with num grad: 0.0005231231932800956\n",
      "\n",
      "Excepted with an grad: [ 5.69921665  7.5687605   3.79636449 -6.64472639 -3.61377124  7.69996933\n",
      "  9.037617    8.9696162   2.26304092]\n",
      "Received with an grad: [ 5.62035073  6.96888467  3.89586462 -1.04151076 -1.46643766 -7.18097593\n",
      "  4.93247557 18.42302872  3.09986899]\n",
      "Absolute error with an grad: 19.09829954573396\n",
      "Relative error with an grad: 0.9696508115124624\n",
      "\n",
      "Excepted with torch grad: [ 5.69921665  7.5687605   3.79636449 -6.64472639 -3.61377124  7.69996933\n",
      "  9.037617    8.9696162   2.26304092]\n",
      "Received with torch grad: [ 5.69927776  7.568963    3.79567024 -6.6462154  -3.6097806   7.70282278\n",
      "  9.0296975   8.96908984  2.26615944]\n",
      "Absolute error with torch grad: 0.009976569198432508\n",
      "Relative error with torch grad: 0.0005065261646045959\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "\n",
    "#2.a\n",
    "\n",
    "#Parameters\n",
    "X1, Y1, T1, Funcs1 = gen_parameters(gen_excepted(), is_random_N=True)\n",
    "\n",
    "#Function\n",
    "f1 = gen_f(X1, Y1, T1, Funcs1)\n",
    "\n",
    "#x0\n",
    "x0_1 = gen_x0(len(Funcs1))\n",
    "\n",
    "#Calculations\n",
    "optimize_result = optimize_handler(f1, x0_1, Algorithms.DogLeg)\n",
    "received = optimize_result.x\n",
    "\n",
    "print(\"Example 2.a\")\n",
    "print(\"-------------------------\")\n",
    "print_errors(excepted, received)\n",
    "print(\"-------------------------\")\n",
    "print()\n",
    "\n",
    "#2.b\n",
    "\n",
    "#Grads\n",
    "num_grad_f1 = num_grad(f1)\n",
    "an_grad_f1 = an_grad(X1, Y1, T1, Funcs1)\n",
    "torch_grad_f1 = torch_grad(f1)\n",
    "\n",
    "#Calculations\n",
    "with_num_grad = minimize(f1, x0_1, method='CG', jac=num_grad_f1).x\n",
    "with_an_grad = minimize(f1, x0_1, method='CG', jac=an_grad_f1).x\n",
    "with_torch_grad = minimize(f1, x0_1, method='CG', jac=torch_grad_f1).x\n",
    "\n",
    "print(\"Example 2.b\")\n",
    "print(\"-------------------------\")\n",
    "print_errors(excepted, with_num_grad, suf=\"with num grad\")\n",
    "\n",
    "print()\n",
    "\n",
    "print_errors(excepted, with_an_grad, suf=\"with an grad\")\n",
    "\n",
    "print()\n",
    "\n",
    "print_errors(excepted, with_torch_grad, suf=\"with torch grad\")\n",
    "print(\"-------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:54.318277800Z",
     "start_time": "2023-06-20T12:32:47.673340800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "BoundMethods = Enum('BoundMethods', ['Nelder-Mead', 'L-BFGS-B', 'TNC', 'SLSQP', 'Powell', 'trust-constr'])\n",
    "\n",
    "\n",
    "def gen_bounds(arity, left_bound=-20, right_bound=20, delta_between=3):\n",
    "    bounds = []\n",
    "    for _ in range(arity):\n",
    "        min_val = random.randint(left_bound, right_bound)\n",
    "        max_val = random.randint(min_val + delta_between, right_bound + delta_between)\n",
    "        bounds.append([min_val, max_val])\n",
    "\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def gen_start_point_by_bounds(bounds):\n",
    "    x0 = []\n",
    "\n",
    "    for i in range(len(bounds)):\n",
    "        x0.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "\n",
    "    return np.array(x0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:54.375176900Z",
     "start_time": "2023-06-20T12:32:54.318277800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def test_scipy_bounds(f, method, arity, max_shift=2):\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    bounds = gen_bounds(arity)\n",
    "    for i in range(len(bounds)):\n",
    "        print(f\"Changing x{i + 1}'s bound\")\n",
    "\n",
    "        for bound_part in [0, 1]:\n",
    "            sign = 1 if bound_part == 0 else -1\n",
    "            for d in range(min(max_shift, bounds[i][1] - bounds[i][0]) + 1):\n",
    "                if d == 0 and not (i == 0 and bound_part == 0):\n",
    "                    continue\n",
    "\n",
    "                bounds[i][bound_part] += sign * d\n",
    "                x0 = gen_start_point_by_bounds(bounds)\n",
    "\n",
    "                print(\"Bounds:\")\n",
    "                print('\\n'.join([f\"{bounds[j][0]} <= x{j + 1} <= {bounds[j][1]}\" for j in range(len(bounds))]))\n",
    "                metrics = get_metrics(lambda *_: minimize(f, x0, method=method, bounds=bounds))\n",
    "                bounds[i][bound_part] -= sign * d\n",
    "\n",
    "                print(f\"Method: {method}\\nResult: {metrics.result.x}\\nMemory: {metrics.mem}\\nTime: {metrics.time}\")\n",
    "                print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:54.375176900Z",
     "start_time": "2023-06-20T12:32:54.343958500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-12 <= x1 <= 0\n",
      "4 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 4.]\n",
      "Memory: 7229\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 0\n",
      "4 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 4.]\n",
      "Memory: 7245\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-10 <= x1 <= 0\n",
      "4 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 4.]\n",
      "Memory: 7482\n",
      "Time: 0.015621185302734375\n",
      "\n",
      "Bounds:\n",
      "-12 <= x1 <= -1\n",
      "4 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [-1.  4.]\n",
      "Memory: 7482\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-12 <= x1 <= -2\n",
      "4 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [-2.  4.]\n",
      "Memory: 7332\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-12 <= x1 <= 0\n",
      "5 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 5.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-12 <= x1 <= 0\n",
      "6 <= x2 <= 11\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 6.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-12 <= x1 <= 0\n",
      "4 <= x2 <= 10\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 4.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-12 <= x1 <= 0\n",
      "4 <= x2 <= 9\n",
      "Method: Nelder-Mead\n",
      "Result: [0. 4.]\n",
      "Memory: 7498\n",
      "Time: 0.015635013580322266\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-5 <= x1 <= 3\n",
      "19 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [3.92368181e-07 1.90000000e+01]\n",
      "Memory: 4257\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-4 <= x1 <= 3\n",
      "19 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [-1.91974006e-07  1.90000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-3 <= x1 <= 3\n",
      "19 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [-1.26982517e-07  1.90000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 2\n",
      "19 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [1.45077639e-06 1.90000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 1\n",
      "19 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [-1.2347803e-06  1.9000000e+01]\n",
      "Memory: 8273\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-5 <= x1 <= 3\n",
      "20 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [1.20159797e-06 2.00000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 3\n",
      "21 <= x2 <= 23\n",
      "Method: L-BFGS-B\n",
      "Result: [-5.65027407e-08  2.10000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.015621185302734375\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 3\n",
      "19 <= x2 <= 22\n",
      "Method: L-BFGS-B\n",
      "Result: [7.69646401e-07 1.90000000e+01]\n",
      "Memory: 8425\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 3\n",
      "19 <= x2 <= 21\n",
      "Method: L-BFGS-B\n",
      "Result: [2.4490779e-07 1.9000000e+01]\n",
      "Memory: 8457\n",
      "Time: 0.0\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-6 <= x1 <= 12\n",
      "17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [2.01872968e-06 1.70000000e+01]\n",
      "Memory: 8146\n",
      "Time: 0.022159576416015625\n",
      "\n",
      "Bounds:\n",
      "-5 <= x1 <= 12\n",
      "17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-1.11574192e-06  1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-4 <= x1 <= 12\n",
      "17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [6.77228247e-08 1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-6 <= x1 <= 11\n",
      "17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-2.54559936e-07  1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.016257762908935547\n",
      "\n",
      "Bounds:\n",
      "-6 <= x1 <= 10\n",
      "17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-4.03031604e-08  1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-6 <= x1 <= 12\n",
      "18 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [7.04723714e-07 1.80000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.015306711196899414\n",
      "\n",
      "Bounds:\n",
      "-6 <= x1 <= 12\n",
      "19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [5.98697011e-07 1.90000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-6 <= x1 <= 12\n",
      "17 <= x2 <= 20\n",
      "Method: TNC\n",
      "Result: [1.28155119e-06 1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.028594493865966797\n",
      "\n",
      "Bounds:\n",
      "-6 <= x1 <= 12\n",
      "17 <= x2 <= 19\n",
      "Method: TNC\n",
      "Result: [2.47850358e-07 1.70000000e+01]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "19 <= x1 <= 22\n",
      "10 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [19. 10.]\n",
      "Memory: 12225\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "20 <= x1 <= 22\n",
      "10 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [20. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "21 <= x1 <= 22\n",
      "10 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [21. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "19 <= x1 <= 21\n",
      "10 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [19. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "19 <= x1 <= 20\n",
      "10 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [19. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "19 <= x1 <= 22\n",
      "11 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [19. 11.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "19 <= x1 <= 22\n",
      "12 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [19. 12.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "19 <= x1 <= 22\n",
      "10 <= x2 <= 21\n",
      "Method: SLSQP\n",
      "Result: [19. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "19 <= x1 <= 22\n",
      "10 <= x2 <= 20\n",
      "Method: SLSQP\n",
      "Result: [19. 10.]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "2 <= x1 <= 18\n",
      "6 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [2.         6.00007799]\n",
      "Memory: 4246\n",
      "Time: 0.015626907348632812\n",
      "\n",
      "Bounds:\n",
      "3 <= x1 <= 18\n",
      "6 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [3.00000001 6.000366  ]\n",
      "Memory: 8190\n",
      "Time: 0.008514642715454102\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 18\n",
      "6 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [4.00000003 6.00007312]\n",
      "Memory: 6637\n",
      "Time: 0.025355815887451172\n",
      "\n",
      "Bounds:\n",
      "2 <= x1 <= 17\n",
      "6 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [2.00000001 6.0003682 ]\n",
      "Memory: 7797\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "2 <= x1 <= 16\n",
      "6 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [2.00005012 6.        ]\n",
      "Memory: 5950\n",
      "Time: 0.03125309944152832\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "2 <= x1 <= 18\n",
      "7 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [2.00042546 7.00000001]\n",
      "Memory: 7313\n",
      "Time: 0.015625\n",
      "\n",
      "Bounds:\n",
      "2 <= x1 <= 18\n",
      "8 <= x2 <= 17\n",
      "Method: Powell\n",
      "Result: [2.00006444 8.        ]\n",
      "Memory: 8360\n",
      "Time: 0.015624523162841797\n",
      "\n",
      "Bounds:\n",
      "2 <= x1 <= 18\n",
      "6 <= x2 <= 16\n",
      "Method: Powell\n",
      "Result: [2.00000004 6.0000669 ]\n",
      "Memory: 8189\n",
      "Time: 0.02767467498779297\n",
      "\n",
      "Bounds:\n",
      "2 <= x1 <= 18\n",
      "6 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [2.00037177 6.00000001]\n",
      "Memory: 8246\n",
      "Time: 0.01563096046447754\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "10 <= x1 <= 17\n",
      "-13 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [1.00000000e+01 1.88737284e-07]\n",
      "Memory: 26723\n",
      "Time: 0.13673710823059082\n",
      "\n",
      "Bounds:\n",
      "11 <= x1 <= 17\n",
      "-13 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [ 1.10000000e+01 -2.36765659e-08]\n",
      "Memory: 26772\n",
      "Time: 0.14629507064819336\n",
      "\n",
      "Bounds:\n",
      "12 <= x1 <= 17\n",
      "-13 <= x2 <= 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vryab\\Main\\IT\\ITMO\\opt-methods\\opt-venv\\lib\\site-packages\\scipy\\optimize\\_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: trust-constr\n",
      "Result: [1.20000000e+01 2.20151182e-07]\n",
      "Memory: 35504\n",
      "Time: 0.18036723136901855\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 16\n",
      "-13 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [1.00000000e+01 7.18876896e-08]\n",
      "Memory: 29796\n",
      "Time: 0.2585866451263428\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 15\n",
      "-13 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [ 1.00000000e+01 -1.71411847e-07]\n",
      "Memory: 27053\n",
      "Time: 0.14277243614196777\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "10 <= x1 <= 17\n",
      "-12 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [ 1.00000000e+01 -1.10806458e-07]\n",
      "Memory: 27147\n",
      "Time: 0.13990187644958496\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 17\n",
      "-11 <= x2 <= 7\n",
      "Method: trust-constr\n",
      "Result: [ 1.00000000e+01 -1.21413147e-07]\n",
      "Memory: 27235\n",
      "Time: 0.11408162117004395\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 17\n",
      "-13 <= x2 <= 6\n",
      "Method: trust-constr\n",
      "Result: [ 1.00000000e+01 -8.83186165e-08]\n",
      "Memory: 28957\n",
      "Time: 0.12190651893615723\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 17\n",
      "-13 <= x2 <= 5\n",
      "Method: trust-constr\n",
      "Result: [1.00000000e+01 2.48659316e-08]\n",
      "Memory: 27048\n",
      "Time: 0.12375020980834961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "\n",
    "for bound_method in BoundMethods:\n",
    "    test_scipy_bounds(f, bound_method.name.replace(\"BoundMethods\", \"\"), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:56.036257300Z",
     "start_time": "2023-06-20T12:32:54.359555800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T12:32:56.056794400Z",
     "start_time": "2023-06-20T12:32:56.036257300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
