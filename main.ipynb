{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from scipy.optimize import minimize, least_squares\n",
    "from torch import float64\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from linreg import Methods, LearningRateScheduling\n",
    "\n",
    "\n",
    "def optimizer_handler(method, params, lr, beta_1=0.9, beta_2=0.999, factor=10):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return torch.optim.SGD(params, lr)\n",
    "        case Methods.Momentum:\n",
    "            return torch.optim.SGD(params, lr, beta_1)\n",
    "        case Methods.AdaGrad:\n",
    "            return torch.optim.Adagrad(params, lr * factor)\n",
    "        case Methods.RMSprop:\n",
    "            return torch.optim.RMSprop(params, lr, alpha=beta_2)\n",
    "        case Methods.Adam:\n",
    "            return torch.optim.Adam(params, lr, betas=(beta_1, beta_2))\n",
    "        case Methods.Nesterov:\n",
    "            return torch.optim.SGD(params, lr, nesterov=True, momentum=beta_1)\n",
    "\n",
    "\n",
    "def lr_scheduler_handler(optimizer, lrs, lr, epoch_size=30):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda *_: 1)\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_size, gamma=0.75)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "\n",
    "class TorchLinearRegression:\n",
    "    def __init__(self, T, X, Y, W=None):\n",
    "        self.T_funcs = T\n",
    "        self.T = torch.tensor([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))], dtype=float64).reshape(\n",
    "            len(X), len(T))\n",
    "        self.X = torch.tensor(X, dtype=float64)\n",
    "        self.Y = torch.tensor(Y, dtype=float64)\n",
    "        self.loss_f = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.refresh(W)\n",
    "\n",
    "    def optimize(self, method=Methods.Classic, lr=0.01, lrs=LearningRateScheduling.Classic, max_steps=1500):\n",
    "        optimizer = optimizer_handler(method, [self.W], lr)\n",
    "        scheduler = lr_scheduler_handler(optimizer, lrs, lr=lr)\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            optimizer.zero_grad()\n",
    "            self.loss(self.W, is_no_grad=False)\n",
    "            optimizer.step()\n",
    "            self.W_points.append(self.W.clone().detach().numpy())\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Method: {method.name} | LRS: {lrs} | LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        return self.W\n",
    "\n",
    "    def loss(self, W, is_no_grad=True):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W).reshape(len(self.T_funcs), 1)\n",
    "            W = Variable(torch.tensor(W, dtype=float64), requires_grad=True)\n",
    "        if is_no_grad:\n",
    "            with torch.no_grad():\n",
    "                loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "                return float(loss_val) * len(self.X)\n",
    "        else:\n",
    "            loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "            loss_val.backward()\n",
    "            return float(loss_val)\n",
    "\n",
    "    def refresh(self, W=None):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W, dtype=float64).reshape(len(self.T_funcs), 1)\n",
    "        if W is None:\n",
    "            W = torch.randn(len(self.T_funcs), 1, dtype=float64)\n",
    "        self.W = Variable(torch.tensor(W), requires_grad=True)\n",
    "        self.W_points = [torch.clone(self.W).detach().numpy()]\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (torch.linalg.inv(torch.t(self.T) @ self.T) @ torch.t(self.T)) @ self.Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def visualise(f, points, title, x_label=\"x\", y_label=\"y\"):\n",
    "    values = np.transpose(np.array(points))\n",
    "    X = np.linspace(min(values[0]) - 10, max(values[0]) + 10, 100)\n",
    "    Y = np.linspace(min(values[1]) - 10, max(values[1]) + 10, 100)\n",
    "    Z = [[f(np.array([X[i], Y[j]])) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 80)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.title(title)\n",
    "    plt.legend(['Route', 'Start point', 'End point'])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_errors(excepted, received, suf=\"\"):\n",
    "    print(\"Excepted \" + suf + \": \" + str(excepted))\n",
    "    print(\"Received \" + suf + \": \" + str(received))\n",
    "    print(\"Absolute error \" + suf + \": \" + str(np.linalg.norm(excepted - received)))\n",
    "    print(\"Relative error \" + suf + \": \" + str(np.linalg.norm(excepted - received) / np.linalg.norm(excepted)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from linreg import gen_linear_reg, visualise_approximation, sgd_handler\n",
    "\n",
    "\n",
    "def linreg_errors(linreg, torch_linreg):\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    expected_linreg_loss = linreg.loss(linreg.analytical_solution())\n",
    "    linreg_loss = linreg.loss(linreg.W)\n",
    "    print_errors(expected_linreg_loss, linreg_loss, suf=\"loss\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    expected_torch_linreg_loss = torch_linreg.loss(torch_linreg.analytical_solution())\n",
    "    torch_linreg_loss = torch_linreg.loss(torch_linreg.W)\n",
    "    print_errors(expected_torch_linreg_loss, torch_linreg_loss, suf=\"torch loss\")\n",
    "\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "\n",
    "def test_sgd_variants(errors=False):\n",
    "    count_2_arity = 1\n",
    "    count_other_arity = 1\n",
    "    left_coeffs_border = -2.\n",
    "    right_coeffs_border = 2.\n",
    "    left_x_border = -2.\n",
    "    right_x_border = 2.\n",
    "    deviation = 3.\n",
    "\n",
    "    for i in range(count_2_arity + count_other_arity):\n",
    "        arity = 2 if i < count_2_arity else random.randint(3, 8)\n",
    "        num_train_points = random.randint(50, 100)\n",
    "        start_point = np.array([float(random.randint(15, 30)) for i in range(arity)])\n",
    "        linreg = gen_linear_reg(\n",
    "            arity - 1, num_train_points,\n",
    "            left_coeffs_border, right_coeffs_border,\n",
    "            left_x_border, right_x_border,\n",
    "            deviation\n",
    "        )\n",
    "        torch_linreg = TorchLinearRegression(linreg.T_funcs, linreg.X, linreg.Y, torch.tensor(start_point))\n",
    "        self_scheduled = (Methods.AdaGrad, Methods.Adam, Methods.RMSprop)\n",
    "\n",
    "        for method in Methods:\n",
    "            lr = 0.01\n",
    "            if method in self_scheduled:\n",
    "                lr = 0.1\n",
    "                linreg.refresh(start_point)\n",
    "                sgd_handler(linreg, lambda *_: lr, method, store_points=True)\n",
    "                title = 'OUR ' + method.name\n",
    "                if len(linreg.T_funcs) == 2:\n",
    "                    visualise(linreg.loss, linreg.W_points, title)\n",
    "                visualise_approximation(linreg, title)\n",
    "\n",
    "                title = 'ZAPADNOE ' + method.name\n",
    "                torch_linreg.refresh(torch.tensor(start_point))\n",
    "                torch_linreg.optimize(method, lr=lr)\n",
    "                if len(torch_linreg.T_funcs) == 2:\n",
    "                    visualise(torch_linreg.loss, torch_linreg.W_points, title)\n",
    "                visualise_approximation(torch_linreg, title)\n",
    "\n",
    "                if errors:\n",
    "                    linreg_errors(linreg, torch_linreg)\n",
    "\n",
    "test_sgd_variants(errors=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def derivative(f, x, i, delt=0.0001):\n",
    "    x_1 = np.copy(x)\n",
    "    x_2 = np.copy(x)\n",
    "    x_1[i] += delt\n",
    "    x_2[i] -= delt\n",
    "    y_1 = f(x_1)\n",
    "    y_2 = f(x_2)\n",
    "    return (y_1 - y_2) / (2 * delt)\n",
    "\n",
    "\n",
    "def grad(f, delt=0.01):\n",
    "    def grad_calc(x):\n",
    "        array = []\n",
    "        for i in range(len(x)):\n",
    "            array.append(derivative(f, x, i, delt))\n",
    "        return np.array(array)\n",
    "\n",
    "    return grad_calc\n",
    "\n",
    "\n",
    "def hessian(f):\n",
    "    def calc(x):\n",
    "        B = np.asarray([[0. for _ in range(len(x))] for _ in range(len(x))])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                B[i][j] = derivative(lambda x_tmp: derivative(f, x_tmp, j), x, i)\n",
    "        return B\n",
    "\n",
    "    return calc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Algorithms = Enum('Methods', ['Newton', 'DogLeg', 'BFGS', 'LBFGS'])\n",
    "\n",
    "\n",
    "def optimize_handler(fun, x0, algorithm=Algorithms.Newton):\n",
    "    match algorithm:\n",
    "        case Algorithms.Newton:\n",
    "            return least_squares(fun, x0)\n",
    "        case Algorithms.DogLeg:\n",
    "            return minimize(fun, x0, method='dogleg', jac=grad(fun), hess=hessian(fun))\n",
    "        case Algorithms.BFGS:\n",
    "            return minimize(fun, x0, method='BFGS')\n",
    "        case Algorithms.LBFGS:\n",
    "            return minimize(fun, x0, method='L-BFGS-B')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#gen random excepted coeffs\n",
    "def gen_excepted(lb=-10, rb=10, min_size=2, max_size=10):\n",
    "    return np.random.uniform(lb, rb, size=(np.random.randint(min_size, max_size)))\n",
    "\n",
    "\n",
    "#gen parameters with poly functions of points\n",
    "def gen_parameters(M, N=100, deviation=0.01, is_random_N=False, lb_N=50, rb_N=200):\n",
    "    if is_random_N:\n",
    "        N = np.random.randint(lb_N, rb_N)\n",
    "\n",
    "    noise = torch.randn(N, 1, dtype=float64) * deviation\n",
    "    powers = [(M - 1 - i) for i in range(M)]\n",
    "    Funcs = np.array([lambda x, i=i: (x ** powers[i]) for i in range(M)])\n",
    "    X = torch.randn(N, 1, dtype=float64)\n",
    "    Y = sum([float(excepted[i]) * Funcs[i](X) for i in range(M)]) + noise\n",
    "    T = torch.zeros(len(X), len(Funcs), dtype=float64)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Funcs)):\n",
    "            T.data[i, j] = Funcs[j](X.data[i])\n",
    "\n",
    "    return (X, Y, T, Funcs)\n",
    "\n",
    "\n",
    "#gen least squares loss function\n",
    "def gen_f(X, Y, T, Funcs):\n",
    "    def f(W):\n",
    "        W1 = None\n",
    "        if isinstance(W, torch.Tensor):\n",
    "            W1 = W\n",
    "        else:\n",
    "            W1 = np.copy(W).reshape(len(Funcs), 1)\n",
    "            W1 = torch.tensor(W1, dtype=float64)\n",
    "        model = T.mm(W1)\n",
    "        mse = nn.MSELoss()\n",
    "        res = mse(model, Y)\n",
    "        return res if isinstance(W, torch.Tensor) else res.item()\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "#gen random x0\n",
    "def gen_x0(len, lb=-20, rb=20):\n",
    "    return np.random.uniform(lb, rb, size=(len))\n",
    "\n",
    "\n",
    "#numeric grad for any function\n",
    "def num_grad(f, delt=0.01):\n",
    "    return grad(f, delt=delt)\n",
    "\n",
    "\n",
    "#analityc grad for any linear function of weights with the least squares problem\n",
    "def an_grad(X, Y, T, Funcs):\n",
    "    M = len(Funcs)\n",
    "\n",
    "    def an_grad_calc(W):\n",
    "        array = []\n",
    "        components_value = ((T @ torch.tensor(W, dtype=float64).reshape(len(W), 1)) - Y)\n",
    "        for i in range(M):\n",
    "            x_fi = Funcs[i](X)\n",
    "            # torch.pow(X, (M - 1 - i))\n",
    "            array.append((x_fi.T @ components_value).item())\n",
    "        return np.array(array)\n",
    "\n",
    "    return an_grad_calc\n",
    "\n",
    "\n",
    "#torch grad for any function with the least squares problem\n",
    "def torch_grad(f):\n",
    "    def torch_grad_calc(W):\n",
    "        x = torch.tensor(W, dtype=float64, requires_grad=True).reshape(len(W), 1)\n",
    "        x.retain_grad()\n",
    "        Q = f(x)\n",
    "        Q.backward()\n",
    "        return x.grad.T.detach().numpy().reshape(len(W))\n",
    "\n",
    "    return torch_grad_calc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Examples\n",
    "\n",
    "\n",
    "#2.a\n",
    "\n",
    "#Excepted\n",
    "excepted = gen_excepted()\n",
    "M = len(excepted)\n",
    "\n",
    "#Parameters\n",
    "X1, Y1, T1, Funcs1 = gen_parameters(M, is_random_N=True)\n",
    "\n",
    "#Function\n",
    "f1 = gen_f(X1, Y1, T1, Funcs1)\n",
    "\n",
    "#x0\n",
    "x0_1 = gen_x0(len(Funcs1))\n",
    "\n",
    "#Calculations\n",
    "optimize_result = optimize_handler(f1, x0_1, Algorithms.DogLeg)\n",
    "received = optimize_result.x\n",
    "\n",
    "print(\"Example 2.a\")\n",
    "print(\"-------------------------\")\n",
    "print_errors(excepted, received)\n",
    "print(\"-------------------------\")\n",
    "print()\n",
    "\n",
    "#2.b\n",
    "\n",
    "#Grads\n",
    "num_grad_f1 = num_grad(f1)\n",
    "an_grad_f1 = an_grad(X1, Y1, T1, Funcs1)\n",
    "torch_grad_f1 = torch_grad(f1)\n",
    "\n",
    "#Calculations\n",
    "with_num_grad = minimize(f1, x0_1, method='CG', jac=num_grad_f1).x\n",
    "with_an_grad = minimize(f1, x0_1, method='CG', jac=an_grad_f1).x\n",
    "with_torch_grad = minimize(f1, x0_1, method='CG', jac=torch_grad_f1).x\n",
    "\n",
    "print(\"Example 2.b\")\n",
    "print(\"-------------------------\")\n",
    "print_errors(excepted, with_num_grad, suf=\"with num grad\")\n",
    "\n",
    "print()\n",
    "\n",
    "print_errors(excepted, with_an_grad, suf=\"with an grad\")\n",
    "\n",
    "print()\n",
    "\n",
    "print_errors(excepted, with_torch_grad, suf=\"with torch grad\")\n",
    "print(\"-------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
