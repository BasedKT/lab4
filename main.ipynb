{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim\n",
    "from scipy.optimize import minimize, least_squares, LinearConstraint, NonlinearConstraint\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from enum import Enum\n",
    "from torch import float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "outputs": [],
   "source": [
    "from linreg import Methods, LearningRateScheduling\n",
    "\n",
    "\n",
    "def optimizer_handler(method, params, lr, beta_1=0.9, beta_2=0.999, factor=10):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return torch.optim.SGD(params, lr)\n",
    "        case Methods.Momentum:\n",
    "            return torch.optim.SGD(params, lr, beta_1)\n",
    "        case Methods.AdaGrad:\n",
    "            return torch.optim.Adagrad(params, lr * factor)\n",
    "        case Methods.RMSprop:\n",
    "            return torch.optim.RMSprop(params, lr, alpha=beta_2)\n",
    "        case Methods.Adam:\n",
    "            return torch.optim.Adam(params, lr, betas=(beta_1, beta_2))\n",
    "        case Methods.Nesterov:\n",
    "            return torch.optim.SGD(params, lr, nesterov=True, momentum=beta_1)\n",
    "\n",
    "\n",
    "class TorchLinearRegression:\n",
    "    def __init__(self, T, X, Y, W=None):\n",
    "        self.T_funcs = T\n",
    "        self.T = torch.tensor([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))], dtype=float64).reshape(\n",
    "            len(X), len(T))\n",
    "        self.X = torch.tensor(X, dtype=float64)\n",
    "        self.Y = torch.tensor(Y, dtype=float64)\n",
    "        self.loss_f = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.refresh(W)\n",
    "\n",
    "    def optimize(self, method=Methods.Classic, lr=0.01, lrs=LearningRateScheduling.Classic, max_steps=1500):\n",
    "        optimizer = optimizer_handler(method, [self.W], lr)\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            optimizer.zero_grad()\n",
    "            self.loss(self.W, is_no_grad=False)\n",
    "            optimizer.step()\n",
    "            self.W_points.append(self.W.clone().detach().numpy())\n",
    "\n",
    "        return self.W\n",
    "\n",
    "    def loss(self, W, is_no_grad=True):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W).reshape(len(self.T_funcs), 1)\n",
    "            W = Variable(torch.tensor(W, dtype=float64), requires_grad=True)\n",
    "        if is_no_grad:\n",
    "            with torch.no_grad():\n",
    "                loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "                return float(loss_val) * len(self.X)\n",
    "        else:\n",
    "            loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "            loss_val.backward()\n",
    "            return float(loss_val)\n",
    "\n",
    "    def refresh(self, W=None):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W, dtype=float64).reshape(len(self.T_funcs), 1)\n",
    "        if W is None:\n",
    "            W = torch.randn(len(self.T_funcs), 1, dtype=float64)\n",
    "        self.W = Variable(torch.tensor(W), requires_grad=True)\n",
    "        self.W_points = [torch.clone(self.W).detach().numpy()]\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (torch.linalg.inv(torch.t(self.T) @ self.T) @ torch.t(self.T)) @ self.Y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def visualise(f, points, title, x_label=\"x\", y_label=\"y\"):\n",
    "    values = np.transpose(np.array(points))\n",
    "    X = np.linspace(min(values[0]) - 10, max(values[0]) + 10, 100)\n",
    "    Y = np.linspace(min(values[1]) - 10, max(values[1]) + 10, 100)\n",
    "    Z = [[f(np.array([X[i], Y[j]])) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 80)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.title(title)\n",
    "    plt.legend(['Route', 'Start point', 'End point'])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_errors(linreg, torch_linreg):\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    expected_linreg_loss = linreg.loss(linreg.analytical_solution())\n",
    "    linreg_loss = linreg.loss(linreg.W)\n",
    "    print(\"Excepted loss: \" + str(expected_linreg_loss))\n",
    "    print(\"Received loss: \" + str(linreg_loss))\n",
    "    print(\"Absolute error loss: \" + str(np.linalg.norm(expected_linreg_loss - linreg_loss)))\n",
    "    print(\"Relative error loss: \" + str(\n",
    "        np.linalg.norm(expected_linreg_loss - linreg_loss) / np.linalg.norm(expected_linreg_loss)))\n",
    "\n",
    "    print()\n",
    "\n",
    "    expected_torch_linreg_loss = torch_linreg.loss(torch_linreg.analytical_solution())\n",
    "    torch_linreg_loss = torch_linreg.loss(torch_linreg.W)\n",
    "    print(\"Excepted torch loss: \" + str(expected_torch_linreg_loss))\n",
    "    print(\"Received linreg torch loss: \" + str(torch_linreg_loss))\n",
    "    print(\"Absolute error torch loss: \" + str(np.linalg.norm(expected_torch_linreg_loss - torch_linreg_loss)))\n",
    "    print(\"Relative error torch loss: \" + str(\n",
    "        np.linalg.norm(expected_torch_linreg_loss - torch_linreg_loss) / np.linalg.norm(expected_torch_linreg_loss)))\n",
    "\n",
    "    print(\"-------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from linreg import gen_linear_reg, visualise_approximation, sgd_handler\n",
    "\n",
    "\n",
    "def test_sgd_variants(errors=False):\n",
    "    count_2_arity = 1\n",
    "    count_other_arity = 0\n",
    "    left_coeffs_border = -3.\n",
    "    right_coeffs_border = 3.\n",
    "    left_x_border = -2.\n",
    "    right_x_border = 2.\n",
    "    deviation = 4.\n",
    "\n",
    "    for i in range(count_2_arity + count_other_arity):\n",
    "        arity = 2 if i < count_2_arity else random.randint(3, 8)\n",
    "        num_train_points = random.randint(50, 100)\n",
    "        start_point = np.array([float(random.randint(15, 30)) for i in range(arity)])\n",
    "        linreg = gen_linear_reg(\n",
    "            arity - 1, num_train_points,\n",
    "            left_coeffs_border, right_coeffs_border,\n",
    "            left_x_border, right_x_border,\n",
    "            deviation\n",
    "        )\n",
    "        torch_linreg = TorchLinearRegression(linreg.T_funcs, linreg.X, linreg.Y, torch.tensor(start_point))\n",
    "        self_scheduled = (Methods.AdaGrad, Methods.Adam, Methods.RMSprop)\n",
    "\n",
    "        for method in Methods:\n",
    "            lr = 0.01\n",
    "            if method in self_scheduled:\n",
    "                lr = 0.1\n",
    "            for lrs in LearningRateScheduling:\n",
    "                if lrs != LearningRateScheduling.Classic and method in self_scheduled:\n",
    "                    continue\n",
    "                linreg.refresh(start_point)\n",
    "                sgd_handler(linreg, lambda *_: lr, method, lrs, store_points=True)\n",
    "                title = 'OUR ' + method.name + ' | ' + lrs.name\n",
    "                if len(linreg.T_funcs == 2):\n",
    "                    visualise_approximation(linreg, title)\n",
    "                visualise(linreg.loss, linreg.W_points, title)\n",
    "\n",
    "                title = 'ZAPADNOE ' + method.name + ' | ' + lrs.name\n",
    "                torch_linreg.refresh(torch.tensor(start_point))\n",
    "                torch_linreg.optimize(method, lr=lr, lrs=lrs)\n",
    "                if (len(torch_linreg.T_funcs) == 2):\n",
    "                    visualise_approximation(torch_linreg, title)\n",
    "                visualise(torch_linreg.loss, torch_linreg.W_points, title)\n",
    "\n",
    "                if errors:\n",
    "                    print_errors(linreg, torch_linreg)\n",
    "\n",
    "\n",
    "test_sgd_variants(errors=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Example 3\n",
    "\n",
    "#Excepted\n",
    "excepted = np.array([-2.34, 8.987, 118.12, 103.1])\n",
    "M = len(excepted)\n",
    "\n",
    "#Parameters\n",
    "N = 100\n",
    "deviation = 0.01\n",
    "noise = torch.randn(N, 1) * deviation\n",
    "powers = [(M - 1 - i) for i in range(M)]\n",
    "Funcs = np.array([lambda x, i=i: (x ** powers[i]) for i in range(M)])\n",
    "X1 = torch.randn(N, 1)\n",
    "Y1 = sum([excepted[i] * Funcs[i](X1) for i in range(M)]) + noise\n",
    "\n",
    "#Calculations\n",
    "lin_reg = TorchLinearRegression(Funcs, X1, Y1)\n",
    "received = lin_reg.optimize().detach().numpy().reshape(1, len(Funcs))[0]\n",
    "print(\"Excepted: \" + str(excepted))\n",
    "print(\"Received: \" + str(received))\n",
    "print(\"Absolute error: \" + str(np.linalg.norm(excepted - received)))\n",
    "print(\"Relative error: \" + str(np.linalg.norm(excepted - received) / np.linalg.norm(excepted)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "outputs": [],
   "source": [
    "def derivative(f, x, i, delt=0.0001):\n",
    "    x_1 = np.copy(x)\n",
    "    x_2 = np.copy(x)\n",
    "    x_1[i] += delt\n",
    "    x_2[i] -= delt\n",
    "    y_1 = f(x_1)\n",
    "    y_2 = f(x_2)\n",
    "    return (y_1 - y_2) / (2 * delt)\n",
    "\n",
    "\n",
    "def grad(f, delt=0.01):\n",
    "    def grad_calc(x):\n",
    "        array = []\n",
    "        for i in range(len(x)):\n",
    "            array.append(derivative(f, x, i, delt))\n",
    "        return np.array(array)\n",
    "\n",
    "    return grad_calc\n",
    "\n",
    "\n",
    "def hessian(f):\n",
    "    def calc(x):\n",
    "        B = np.asarray([[0. for _ in range(len(x))] for _ in range(len(x))])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                B[i][j] = derivative(lambda x_tmp: derivative(f, x_tmp, j), x, i)\n",
    "        return B\n",
    "\n",
    "    return calc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "outputs": [],
   "source": [
    "Algorithms = Enum('Methods', ['Newton', 'DogLeg', 'BFGS', 'LBFGS'])\n",
    "\n",
    "\n",
    "def optimize_handler(fun, x0, algorithm=Algorithms.Newton):\n",
    "    match algorithm:\n",
    "        case Algorithms.Newton:\n",
    "            return least_squares(fun, x0)\n",
    "        case Algorithms.DogLeg:\n",
    "            return minimize(fun, x0, method='dogleg', jac=grad(fun), hess=hessian(fun))\n",
    "        case Algorithms.BFGS:\n",
    "            return minimize(fun, x0, method='BFGS')\n",
    "        case Algorithms.LBFGS:\n",
    "            return minimize(fun, x0, method='L-BFGS-B')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Example 4\n",
    "\n",
    "#Excepted\n",
    "excepted = np.array([-2.34, 8.987, 118.12, 103.1])\n",
    "M = len(excepted)\n",
    "\n",
    "#Parameters\n",
    "N = 100\n",
    "deviation = 0.01\n",
    "noise = torch.randn(N, 1) * deviation\n",
    "powers = [(M - 1 - i) for i in range(M)]\n",
    "Funcs = np.array([lambda x, i=i: (x ** powers[i]) for i in range(M)])\n",
    "X1 = torch.randn(N, 1)\n",
    "Y1 = sum([excepted[i] * Funcs[i](X1) for i in range(M)]) + noise\n",
    "\n",
    "\n",
    "#Function\n",
    "def f1(W):\n",
    "    W1 = np.copy(W).reshape(len(Funcs), 1)\n",
    "    W1 = torch.tensor(W1, dtype=float64)\n",
    "    T = torch.zeros(len(X1), len(Funcs), dtype=float64)\n",
    "    for i in range(len(X1)):\n",
    "        for j in range(len(Funcs)):\n",
    "            T.data[i, j] = Funcs[j](X1.data[i])\n",
    "    model = T.mm(W1)\n",
    "    mse = nn.MSELoss()\n",
    "    return mse(model, Y1).item()\n",
    "\n",
    "\n",
    "#Calculations\n",
    "x0 = torch.randn(M).detach().numpy().reshape(1, len(Funcs))[0]\n",
    "received = optimize_handler(f1, x0, Algorithms.DogLeg).x\n",
    "print(\"Excepted: \" + str(excepted))\n",
    "print(\"Received: \" + str(received))\n",
    "print(\"Absolute error: \" + str(np.linalg.norm(excepted - received)))\n",
    "print(\"Relative error: \" + str(np.linalg.norm(excepted - received) / np.linalg.norm(excepted)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
