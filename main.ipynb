{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.297252200Z",
     "start_time": "2023-06-21T14:07:08.281623300Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from scipy.optimize import minimize, least_squares\n",
    "from torch import float64\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from linreg import Methods, LearningRateScheduling, LinearRegression\n",
    "\n",
    "\n",
    "def optimizer_handler(method, params, lr, beta_1=0.9, beta_2=0.999, factor=10):\n",
    "    match method:\n",
    "        case Methods.Classic:\n",
    "            return torch.optim.SGD(params, lr)\n",
    "        case Methods.Momentum:\n",
    "            return torch.optim.SGD(params, lr, beta_1)\n",
    "        case Methods.AdaGrad:\n",
    "            return torch.optim.Adagrad(params, lr * factor)\n",
    "        case Methods.RMSprop:\n",
    "            return torch.optim.RMSprop(params, lr, alpha=beta_2)\n",
    "        case Methods.Adam:\n",
    "            return torch.optim.Adam(params, lr, betas=(beta_1, beta_2))\n",
    "        case Methods.Nesterov:\n",
    "            return torch.optim.SGD(params, lr, nesterov=True, momentum=beta_1)\n",
    "\n",
    "\n",
    "def lr_scheduler_handler(optimizer, lrs, lr, epoch_size=30):\n",
    "    match lrs:\n",
    "        case LearningRateScheduling.Classic:\n",
    "            return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda *_: 1)\n",
    "        case LearningRateScheduling.Stepwise:\n",
    "            return torch.optim.lr_scheduler.StepLR(optimizer, step_size=epoch_size, gamma=0.75)\n",
    "        case LearningRateScheduling.Exponential:\n",
    "            return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "\n",
    "class TorchLinearRegression:\n",
    "    def __init__(self, T, X, Y, W=None):\n",
    "        self.T_funcs = T\n",
    "        self.T = torch.tensor([T[i % len(T)](X[i // len(T)]) for i in range(len(T) * len(X))], dtype=float64).reshape(\n",
    "            len(X), len(T))\n",
    "        self.X = torch.tensor(X, dtype=float64)\n",
    "        self.Y = torch.tensor(Y, dtype=float64)\n",
    "        self.loss_f = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.refresh(W)\n",
    "\n",
    "    def optimize(self, method=Methods.Classic, lr=0.01, lrs=LearningRateScheduling.Classic, max_steps=1500):\n",
    "        optimizer = optimizer_handler(method, [self.W], lr)\n",
    "        scheduler = lr_scheduler_handler(optimizer, lrs, lr=lr)\n",
    "\n",
    "        for i in range(max_steps):\n",
    "            optimizer.zero_grad()\n",
    "            self.loss(self.W, is_no_grad=False)\n",
    "            optimizer.step()\n",
    "            self.W_points.append(self.W.clone().detach().numpy())\n",
    "            scheduler.step()\n",
    "\n",
    "        return self.W\n",
    "\n",
    "    def loss(self, W, is_no_grad=True):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W).reshape(len(self.T_funcs), 1)\n",
    "            W = Variable(torch.tensor(W, dtype=float64), requires_grad=True)\n",
    "        if is_no_grad:\n",
    "            with torch.no_grad():\n",
    "                loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "                return float(loss_val) * len(self.X)\n",
    "        else:\n",
    "            loss_val = self.loss_f(self.T @ W, self.Y)\n",
    "            loss_val.backward()\n",
    "            return float(loss_val)\n",
    "\n",
    "    def refresh(self, W=None):\n",
    "        if isinstance(W, np.ndarray):\n",
    "            W = np.array(W, dtype=float64).reshape(len(self.T_funcs), 1)\n",
    "        if W is None:\n",
    "            W = torch.randn(len(self.T_funcs), 1, dtype=float64)\n",
    "        self.W = Variable(torch.tensor(W), requires_grad=True)\n",
    "        self.W_points = [torch.clone(self.W).detach().numpy()]\n",
    "\n",
    "    def analytical_solution(self):\n",
    "        return (torch.linalg.inv(torch.t(self.T) @ self.T) @ torch.t(self.T)) @ self.Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.344134100Z",
     "start_time": "2023-06-21T14:07:08.312872600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "from time import time\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self, result, mem, time, other=None):\n",
    "        if other is None:\n",
    "            other = []\n",
    "        self.result = result\n",
    "        self.mem = mem\n",
    "        self.time = time\n",
    "        self.other = other\n",
    "\n",
    "\n",
    "def get_metrics(func):\n",
    "    start = time()\n",
    "    tracemalloc.start()\n",
    "    result = func()\n",
    "    mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end = time()\n",
    "\n",
    "    return Metrics(result, mem[1] - mem[0], end - start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.344134100Z",
     "start_time": "2023-06-21T14:07:08.328497100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from excel import ExcellSaver\n",
    "\n",
    "excel_saver = ExcellSaver()\n",
    "\n",
    "\n",
    "def add_metrics_row(metrics):\n",
    "    global excel_saver\n",
    "\n",
    "    row = []\n",
    "    for pair in metrics.other:\n",
    "        row.append(pair[1])\n",
    "    row.append(metrics.time)\n",
    "    row.append(metrics.mem)\n",
    "\n",
    "    excel_saver.add_row(row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.359766Z",
     "start_time": "2023-06-21T14:07:08.344134100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "from linreg import gen_linear_reg, visualise_approximation, sgd_handler\n",
    "\n",
    "Output = Enum(\"Output\", [\"No\", \"Console\", \"Excel\"])\n",
    "\n",
    "\n",
    "def visualise(f, points, title, x_label=\"x\", y_label=\"y\"):\n",
    "    values = np.transpose(np.array(points))\n",
    "    X = np.linspace(min(values[0]) - 10, max(values[0]) + 10, 100)\n",
    "    Y = np.linspace(min(values[1]) - 10, max(values[1]) + 10, 100)\n",
    "    Z = [[f(np.array([X[i], Y[j]])) for i in range(len(X))] for j in range(len(Y))]\n",
    "    plt.contour(X, Y, Z, 80)\n",
    "\n",
    "    plt.plot(values[0], values[1], marker='.')\n",
    "    plt.plot(values[0][0], values[1][0], 'og')\n",
    "    plt.plot(values[0][-1], values[1][-1], 'or')\n",
    "    plt.title(title)\n",
    "    plt.legend(['Route', 'Start point', 'End point'])\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_abs_rel_errors_info(metrics, excepted, received):\n",
    "    metrics.other.append([\"Absolute Error\", str(np.linalg.norm(excepted - received))])\n",
    "    metrics.other.append([\"Relative Error\", str(np.linalg.norm(excepted - received) / np.linalg.norm(excepted))])\n",
    "\n",
    "\n",
    "def add_linreg_errors_info(metrics, linreg):\n",
    "    expected_linreg_loss = linreg.loss(linreg.analytical_solution())\n",
    "    linreg_loss = linreg.loss(linreg.W)\n",
    "    metrics.other.append([\"Expected Loss\", expected_linreg_loss])\n",
    "    metrics.other.append([\"Actual Loss\", linreg_loss])\n",
    "\n",
    "\n",
    "def test_sgd_variants(output=Output.No):\n",
    "    global excel_saver\n",
    "\n",
    "    if output == Output.Excel:\n",
    "        excel_saver.add_new_sheet([\"Whose\", \"Method\", \"Expected Loss\", \"Actual Loss\", \"Time\", \"Memory\"],\n",
    "                                  \"Test SGD Methods\")\n",
    "\n",
    "    count_2_arity = 1\n",
    "    count_other_arity = 0\n",
    "    left_coeffs_border = -2.\n",
    "    right_coeffs_border = 2.\n",
    "    left_x_border = -2.\n",
    "    right_x_border = 2.\n",
    "    deviation = 3.\n",
    "\n",
    "    for i in range(count_2_arity + count_other_arity):\n",
    "        arity = 2 if i < count_2_arity else random.randint(3, 8)\n",
    "        num_train_points = random.randint(50, 100)\n",
    "        start_point = np.array([float(random.randint(15, 30)) for i in range(arity)])\n",
    "        linreg = gen_linear_reg(\n",
    "            arity - 1, num_train_points,\n",
    "            left_coeffs_border, right_coeffs_border,\n",
    "            left_x_border, right_x_border,\n",
    "            deviation\n",
    "        )\n",
    "        torch_linreg = TorchLinearRegression(linreg.T_funcs, linreg.X, linreg.Y, torch.tensor(start_point))\n",
    "        self_scheduled = (Methods.AdaGrad, Methods.Adam, Methods.RMSprop)\n",
    "\n",
    "        for method in Methods:\n",
    "            lr = 0.01\n",
    "            if method in self_scheduled:\n",
    "                lr = 0.1\n",
    "                linreg.refresh(start_point)\n",
    "                our_metrics = get_metrics(lambda: sgd_handler(linreg, lambda *_: lr, method, store_points=True))\n",
    "                title = 'Our ' + method.name\n",
    "                if len(linreg.T_funcs) == 2:\n",
    "                    visualise(linreg.loss, linreg.W_points, title)\n",
    "                visualise_approximation(linreg, title)\n",
    "\n",
    "                title = 'Torch ' + method.name\n",
    "                torch_linreg.refresh(torch.tensor(start_point))\n",
    "                torch_metrics = get_metrics(lambda: torch_linreg.optimize(method, lr=lr))\n",
    "                if len(torch_linreg.T_funcs) == 2:\n",
    "                    visualise(torch_linreg.loss, torch_linreg.W_points, title)\n",
    "                visualise_approximation(torch_linreg, title)\n",
    "\n",
    "                match output:\n",
    "                    case Output.Excel:\n",
    "                        our_metrics.other.append([\"Whose\", \"Our\"])\n",
    "                        our_metrics.other.append([\"Method\", method.name.replace(\"Methods.\", \"\")])\n",
    "                        add_linreg_errors_info(our_metrics, linreg)\n",
    "                        add_metrics_row(our_metrics)\n",
    "\n",
    "                        torch_metrics.other.append([\"Whose\", \"Torch\"])\n",
    "                        torch_metrics.other.append([\"Method\", method.name.replace(\"Methods.\", \"\")])\n",
    "                        add_linreg_errors_info(torch_metrics, torch_linreg)\n",
    "                        add_metrics_row(torch_metrics)\n",
    "                    case Output.Console:\n",
    "                        pass\n",
    "\n",
    "# test_sgd_variants(Output.Excel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.404034500Z",
     "start_time": "2023-06-21T14:07:08.372910200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def derivative(f, x, i, delt=0.0001):\n",
    "    x_1 = np.copy(x)\n",
    "    x_2 = np.copy(x)\n",
    "    x_1[i] += delt\n",
    "    x_2[i] -= delt\n",
    "    y_1 = f(x_1)\n",
    "    y_2 = f(x_2)\n",
    "    return (y_1 - y_2) / (2 * delt)\n",
    "\n",
    "\n",
    "def grad(f, delt=0.01):\n",
    "    def grad_calc(x):\n",
    "        array = []\n",
    "        for i in range(len(x)):\n",
    "            array.append(derivative(f, x, i, delt))\n",
    "        return np.array(array)\n",
    "\n",
    "    return grad_calc\n",
    "\n",
    "\n",
    "def hessian(f):\n",
    "    def calc(x):\n",
    "        B = np.asarray([[0. for _ in range(len(x))] for _ in range(len(x))])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                B[i][j] = derivative(lambda x_tmp: derivative(f, x_tmp, j), x, i)\n",
    "        return B\n",
    "\n",
    "    return calc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.423904100Z",
     "start_time": "2023-06-21T14:07:08.404034500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "Algorithms = Enum('Algorithms', ['Newton', 'DogLeg', 'BFGS', 'LBFGS'])\n",
    "\n",
    "\n",
    "def optimize_handler(fun, x0, algorithm=Algorithms.Newton):\n",
    "    match algorithm:\n",
    "        case Algorithms.Newton:\n",
    "            return least_squares(fun, x0)\n",
    "        case Algorithms.DogLeg:\n",
    "            return minimize(fun, x0, method='dogleg', jac=grad(fun), hess=hessian(fun))\n",
    "        case Algorithms.BFGS:\n",
    "            return minimize(fun, x0, method='BFGS')\n",
    "        case Algorithms.LBFGS:\n",
    "            return minimize(fun, x0, method='L-BFGS-B')\n",
    "\n",
    "\n",
    "def our_optimize_handler(fun, x0, algorithm=Algorithms.Newton):\n",
    "    match algorithm:\n",
    "        case Algorithms.Newton:\n",
    "            return gauss_newton(x0, fun)\n",
    "        case Algorithms.DogLeg:\n",
    "            return dogleg(fun, x0)\n",
    "        case Algorithms.BFGS:\n",
    "            return bfgs(fun, x0)\n",
    "        case Algorithms.LBFGS:\n",
    "            return lbfgs(fun, x0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.447110Z",
     "start_time": "2023-06-21T14:07:08.427895200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#gen random excepted coeffs\n",
    "def gen_excepted(lb=-10, rb=10, min_size=2, max_size=10):\n",
    "    return np.random.uniform(lb, rb, size=(np.random.randint(min_size, max_size)))\n",
    "\n",
    "\n",
    "#gen parameters with poly functions of points\n",
    "def gen_parameters(excepted, N=100, deviation=0.01, is_random_N=False, lb_N=50, rb_N=200):\n",
    "    M = len(excepted)\n",
    "    if is_random_N:\n",
    "        N = np.random.randint(lb_N, rb_N)\n",
    "\n",
    "    noise = torch.randn(N, 1, dtype=float64) * deviation\n",
    "    powers = [(M - 1 - i) for i in range(M)]\n",
    "    Funcs = np.array([lambda x, i=i: (x ** powers[i]) for i in range(M)])\n",
    "    X = torch.randn(N, 1, dtype=float64)\n",
    "    Y = sum([float(excepted[i]) * Funcs[i](X) for i in range(M)]) + noise\n",
    "    T = torch.zeros(len(X), len(Funcs), dtype=float64)\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Funcs)):\n",
    "            T.data[i, j] = Funcs[j](X.data[i])\n",
    "\n",
    "    return (X, Y, T, Funcs)\n",
    "\n",
    "\n",
    "#gen least squares loss function\n",
    "def gen_f(X, Y, T, Funcs):\n",
    "    def f(W):\n",
    "        W1 = None\n",
    "        if isinstance(W, torch.Tensor):\n",
    "            W1 = W\n",
    "        else:\n",
    "            W1 = np.copy(W).reshape(len(Funcs), 1)\n",
    "            W1 = torch.tensor(W1, dtype=float64)\n",
    "        model = T.mm(W1)\n",
    "        mse = nn.MSELoss()\n",
    "        res = mse(model, Y)\n",
    "        return res if isinstance(W, torch.Tensor) else res.item()\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "#gen random x0\n",
    "def gen_x0(len, lb=-20, rb=20):\n",
    "    return np.random.uniform(lb, rb, size=len)\n",
    "\n",
    "\n",
    "#numeric grad for any function\n",
    "def num_grad(f, delt=0.01):\n",
    "    return grad(f, delt=delt)\n",
    "\n",
    "\n",
    "#analityc grad for any linear function of weights with the least squares problem\n",
    "def an_grad(X, Y, T, Funcs):\n",
    "    M = len(Funcs)\n",
    "\n",
    "    def an_grad_calc(W):\n",
    "        array = []\n",
    "        components_value = ((T @ torch.tensor(W, dtype=float64).reshape(len(W), 1)) - Y)\n",
    "        for i in range(M):\n",
    "            x_fi = Funcs[i](X)\n",
    "            array.append((x_fi.T @ components_value).item())\n",
    "        return np.array(array)\n",
    "\n",
    "    return an_grad_calc\n",
    "\n",
    "\n",
    "#torch grad for any function with the least squares problem\n",
    "def torch_grad(f):\n",
    "    def torch_grad_calc(W):\n",
    "        x = torch.tensor(W, dtype=float64, requires_grad=True).reshape(len(W), 1)\n",
    "        x.retain_grad()\n",
    "        Q = f(x)\n",
    "        Q.backward()\n",
    "        return x.grad.T.detach().numpy().reshape(len(W))\n",
    "\n",
    "    return torch_grad_calc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.464637700Z",
     "start_time": "2023-06-21T14:07:08.439655900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from trustreg import gauss_newton, dogleg, bfgs, lbfgs\n",
    "\n",
    "\n",
    "def test_pytorch_grad(output: Output.No):\n",
    "    global excel_saver\n",
    "\n",
    "    if output == Output.Excel:\n",
    "        excel_saver.add_new_sheet([\"Whose\", \"Algorithm\", \"Absolute Error\", \"Relative Error\", \"Time\", \"Memory\"])\n",
    "\n",
    "    count_examples = 1\n",
    "    for _ in range(count_examples):\n",
    "        expected = gen_excepted(max_size=6)\n",
    "        X, Y, T, Funcs = gen_parameters(expected, is_random_N=True, deviation=6)\n",
    "        f = gen_f(X, Y, T, Funcs)\n",
    "        x0 = gen_x0(len(Funcs))\n",
    "\n",
    "        def tmp(i):\n",
    "            def culc(W):\n",
    "                # print(W.shape)\n",
    "                # print(T.shape)\n",
    "                return (torch.tensor(W) @ torch.t(T[i]) - Y[i]).item()\n",
    "\n",
    "            return culc\n",
    "\n",
    "        for algorithm in Algorithms:\n",
    "            scipy_metrics = get_metrics(lambda: optimize_handler(f, x0, algorithm))\n",
    "            our_metrics = None\n",
    "            if algorithm == Algorithms.Newton:\n",
    "                # continue\n",
    "                funcs = [tmp(i) for i in range(len(X))]\n",
    "                our_metrics = get_metrics(lambda: our_optimize_handler(funcs, x0, algorithm))\n",
    "            else:\n",
    "                our_metrics = get_metrics(lambda: our_optimize_handler(f, x0, algorithm))\n",
    "\n",
    "            linreg = LinearRegression(Funcs, our_metrics.result, X.detach().numpy(), Y.detach().numpy())\n",
    "            scipy_linreg = LinearRegression(Funcs, scipy_metrics.result.x, X.detach().numpy(), Y.detach().numpy())\n",
    "\n",
    "            visualise_approximation(linreg, f\"Our {algorithm.name.replace('Algorithms', '')}\")\n",
    "            visualise_approximation(scipy_linreg, f\"SciPy {algorithm.name.replace('Algorithms', '')}\")\n",
    "\n",
    "            match output:\n",
    "                case Output.Excel:\n",
    "                    our_metrics.other.append([\"Whose\", \"Our\"])\n",
    "                    our_metrics.other.append([\"Algorithm\", algorithm.name.replace(\"Algorithm.\", \"\")])\n",
    "                    add_abs_rel_errors_info(our_metrics, expected, our_metrics.result)\n",
    "                    add_metrics_row(our_metrics)\n",
    "\n",
    "                    scipy_metrics.other.append([\"Whose\", \"SciPy\"])\n",
    "                    scipy_metrics.other.append([\"Algorithm\", algorithm.name.replace(\"Algorithm.\", \"\")])\n",
    "                    add_abs_rel_errors_info(scipy_metrics, expected, scipy_metrics.result.x)\n",
    "                    add_metrics_row(scipy_metrics)\n",
    "\n",
    "\n",
    "# test_pytorch_grad(Output.Excel)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:08.488527Z",
     "start_time": "2023-06-21T14:07:08.465638300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def test_pytorch_grad(output=Output.No):\n",
    "    global excel_saver\n",
    "\n",
    "    if output == Output.Excel:\n",
    "        excel_saver.add_new_sheet([\"Method Grad Calc\", \"Absolute Error\", \"Relative Error\", \"Time\", \"Memory\"],\n",
    "                                  \"Pytorch Gradient Calculation\")\n",
    "\n",
    "    count_examples = 50\n",
    "    for _ in range(count_examples):\n",
    "        expected = gen_excepted(max_size=8)\n",
    "        X, Y, T, Funcs = gen_parameters(expected, is_random_N=True, deviation=6)\n",
    "        f = gen_f(X, Y, T, Funcs)\n",
    "        x0 = gen_x0(len(Funcs))\n",
    "        num_grad_f = num_grad(f)\n",
    "        an_grad_f = an_grad(X, Y, T, Funcs)\n",
    "        torch_grad_f = torch_grad(f)\n",
    "\n",
    "        num_metrics = get_metrics(lambda: minimize(f, x0, method='CG', jac=num_grad_f).x)\n",
    "        an_metrics = get_metrics(lambda: minimize(f, x0, method='CG', jac=an_grad_f).x)\n",
    "        torch_metrics = get_metrics(lambda: minimize(f, x0, method='CG', jac=torch_grad_f).x)\n",
    "\n",
    "        match output:\n",
    "            case Output.Excel:\n",
    "                num_metrics.other.append([\"Method Grad Calc\", \"Numerical\"])\n",
    "                add_abs_rel_errors_info(num_metrics, expected, num_metrics.result)\n",
    "                add_metrics_row(num_metrics)\n",
    "\n",
    "                an_metrics.other.append([\"Method Grad Calc\", \"Analytic\"])\n",
    "                add_abs_rel_errors_info(an_metrics, expected, an_metrics.result)\n",
    "                add_metrics_row(an_metrics)\n",
    "\n",
    "                torch_metrics.other.append([\"Method Grad Calc\", \"PyTorch\"])\n",
    "                add_abs_rel_errors_info(torch_metrics, expected, torch_metrics.result)\n",
    "                add_metrics_row(torch_metrics)\n",
    "\n",
    "\n",
    "test_pytorch_grad(Output.Excel)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:40.561934Z",
     "start_time": "2023-06-21T14:07:08.486353500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "BoundMethods = Enum('BoundMethods', ['Nelder-Mead', 'L-BFGS-B', 'TNC', 'SLSQP', 'Powell', 'trust-constr'])\n",
    "\n",
    "\n",
    "def gen_bounds(arity, left_bound=-20, right_bound=20, delta_between=3):\n",
    "    bounds = []\n",
    "    for _ in range(arity):\n",
    "        min_val = random.randint(left_bound, right_bound)\n",
    "        max_val = random.randint(min_val + delta_between, right_bound + delta_between)\n",
    "        bounds.append([min_val, max_val])\n",
    "\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def gen_start_point_by_bounds(bounds):\n",
    "    x0 = []\n",
    "\n",
    "    for i in range(len(bounds)):\n",
    "        x0.append(random.uniform(bounds[i][0], bounds[i][1]))\n",
    "\n",
    "    return np.array(x0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:40.605723600Z",
     "start_time": "2023-06-21T14:07:40.568418Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def test_scipy_bounds(f, method, arity, max_shift=2):\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    bounds = gen_bounds(arity)\n",
    "    for i in range(len(bounds)):\n",
    "        print(f\"Changing x{i + 1}'s bound\")\n",
    "\n",
    "        for bound_part in [0, 1]:\n",
    "            sign = 1 if bound_part == 0 else -1\n",
    "            for d in range(min(max_shift, bounds[i][1] - bounds[i][0]) + 1):\n",
    "                if d == 0 and not (i == 0 and bound_part == 0):\n",
    "                    continue\n",
    "\n",
    "                bounds[i][bound_part] += sign * d\n",
    "                x0 = gen_start_point_by_bounds(bounds)\n",
    "\n",
    "                print(\"Bounds:\")\n",
    "                print('\\n'.join([f\"{bounds[j][0]} <= x{j + 1} <= {bounds[j][1]}\" for j in range(len(bounds))]))\n",
    "                metrics = get_metrics(lambda *_: minimize(f, x0, method=method, bounds=bounds))\n",
    "                bounds[i][bound_part] -= sign * d\n",
    "\n",
    "                print(f\"Method: {method}\\nResult: {metrics.result.x}\\nMemory: {metrics.mem}\\nTime: {metrics.time}\")\n",
    "                print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:40.606728400Z",
     "start_time": "2023-06-21T14:07:40.593730700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "15 <= x1 <= 19\n",
      "6 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  6.]\n",
      "Memory: 6956\n",
      "Time: 0.005004167556762695\n",
      "\n",
      "Bounds:\n",
      "16 <= x1 <= 19\n",
      "6 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [16.  6.]\n",
      "Memory: 7466\n",
      "Time: 0.0009899139404296875\n",
      "\n",
      "Bounds:\n",
      "17 <= x1 <= 19\n",
      "6 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [17.  6.]\n",
      "Memory: 7539\n",
      "Time: 0.0004115104675292969\n",
      "\n",
      "Bounds:\n",
      "15 <= x1 <= 18\n",
      "6 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  6.]\n",
      "Memory: 7300\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "15 <= x1 <= 17\n",
      "6 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  6.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "15 <= x1 <= 19\n",
      "7 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  7.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "15 <= x1 <= 19\n",
      "8 <= x2 <= 19\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  8.]\n",
      "Memory: 7482\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "15 <= x1 <= 19\n",
      "6 <= x2 <= 18\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  6.]\n",
      "Memory: 7498\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "15 <= x1 <= 19\n",
      "6 <= x2 <= 17\n",
      "Method: Nelder-Mead\n",
      "Result: [15.  6.]\n",
      "Memory: 7554\n",
      "Time: 0.0\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "10 <= x1 <= 13\n",
      "-16 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -11.]\n",
      "Memory: 4096\n",
      "Time: 0.01566624641418457\n",
      "\n",
      "Bounds:\n",
      "11 <= x1 <= 13\n",
      "-16 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 11. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "12 <= x1 <= 13\n",
      "-16 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 12. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.0009953975677490234\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 12\n",
      "-16 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.00101470947265625\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 11\n",
      "-16 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.0009968280792236328\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "10 <= x1 <= 13\n",
      "-15 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.00099945068359375\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 13\n",
      "-14 <= x2 <= -11\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -11.]\n",
      "Memory: 8432\n",
      "Time: 0.0009870529174804688\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 13\n",
      "-16 <= x2 <= -12\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -12.]\n",
      "Memory: 8432\n",
      "Time: 0.0020575523376464844\n",
      "\n",
      "Bounds:\n",
      "10 <= x1 <= 13\n",
      "-16 <= x2 <= -13\n",
      "Method: L-BFGS-B\n",
      "Result: [ 10. -13.]\n",
      "Memory: 8432\n",
      "Time: 0.0019850730895996094\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-16 <= x1 <= -9\n",
      "-19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00 -1.32648836e-05]\n",
      "Memory: 7994\n",
      "Time: 0.0069658756256103516\n",
      "\n",
      "Bounds:\n",
      "-15 <= x1 <= -9\n",
      "-19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00  3.14974542e-07]\n",
      "Memory: 8242\n",
      "Time: 0.005210161209106445\n",
      "\n",
      "Bounds:\n",
      "-14 <= x1 <= -9\n",
      "-19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00 -5.27715309e-08]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= -10\n",
      "-19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-1.0000000e+01 -2.3508705e-08]\n",
      "Memory: 8242\n",
      "Time: 0.015649080276489258\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= -11\n",
      "-19 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-1.10000000e+01 -2.21376895e-08]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-16 <= x1 <= -9\n",
      "-18 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00 -2.87145485e-07]\n",
      "Memory: 8090\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= -9\n",
      "-17 <= x2 <= 21\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00 -2.17389765e-07]\n",
      "Memory: 8090\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= -9\n",
      "-19 <= x2 <= 20\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00  1.49453454e-07]\n",
      "Memory: 8242\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= -9\n",
      "-19 <= x2 <= 19\n",
      "Method: TNC\n",
      "Result: [-9.00000000e+00 -1.86073673e-06]\n",
      "Memory: 8090\n",
      "Time: 0.015624761581420898\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-17 <= x1 <= 22\n",
      "0 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [-7.45231616e-09  1.49018434e-24]\n",
      "Memory: 12073\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-16 <= x1 <= 22\n",
      "0 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [-2.10406998e-08  3.05401872e-14]\n",
      "Memory: 12169\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-15 <= x1 <= 22\n",
      "0 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [-1.04400701e-08  2.10179775e-14]\n",
      "Memory: 12169\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-17 <= x1 <= 21\n",
      "0 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [3.46717766e-10 4.14183933e-17]\n",
      "Memory: 12353\n",
      "Time: 0.015624284744262695\n",
      "\n",
      "Bounds:\n",
      "-17 <= x1 <= 20\n",
      "0 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [3.38770678e-13 0.00000000e+00]\n",
      "Memory: 12353\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-17 <= x1 <= 22\n",
      "1 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [-1.1933222e-04  1.0000000e+00]\n",
      "Memory: 12169\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-17 <= x1 <= 22\n",
      "2 <= x2 <= 23\n",
      "Method: SLSQP\n",
      "Result: [2.26974879e-08 2.00000000e+00]\n",
      "Memory: 12321\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-17 <= x1 <= 22\n",
      "0 <= x2 <= 22\n",
      "Method: SLSQP\n",
      "Result: [-7.47889060e-09  1.17230805e-24]\n",
      "Memory: 12169\n",
      "Time: 0.0\n",
      "\n",
      "Bounds:\n",
      "-17 <= x1 <= 22\n",
      "0 <= x2 <= 21\n",
      "Method: SLSQP\n",
      "Result: [-2.12449920e-08  1.62217254e-24]\n",
      "Memory: 12169\n",
      "Time: 0.0\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "4 <= x1 <= 21\n",
      "-13 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [4.00000000e+00 3.33205749e-04]\n",
      "Memory: 4226\n",
      "Time: 0.02109837532043457\n",
      "\n",
      "Bounds:\n",
      "5 <= x1 <= 21\n",
      "-13 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [5.00000000e+00 3.36020234e-04]\n",
      "Memory: 8049\n",
      "Time: 0.03306937217712402\n",
      "\n",
      "Bounds:\n",
      "6 <= x1 <= 21\n",
      "-13 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [ 6.00000001e+00 -1.96106772e-04]\n",
      "Memory: 8247\n",
      "Time: 0.008142471313476562\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 20\n",
      "-13 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [ 4.00000000e+00 -4.05089249e-08]\n",
      "Memory: 7955\n",
      "Time: 0.015649795532226562\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 19\n",
      "-13 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [4.000000e+00 1.194652e-05]\n",
      "Memory: 8304\n",
      "Time: 0.0\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "4 <= x1 <= 21\n",
      "-12 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [4.00000000e+00 5.92852865e-10]\n",
      "Memory: 8246\n",
      "Time: 0.015624523162841797\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 21\n",
      "-11 <= x2 <= 15\n",
      "Method: Powell\n",
      "Result: [4.000000e+00 1.486919e-08]\n",
      "Memory: 8745\n",
      "Time: 0.015603303909301758\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 21\n",
      "-13 <= x2 <= 14\n",
      "Method: Powell\n",
      "Result: [4.00000000e+00 1.23930806e-08]\n",
      "Memory: 7182\n",
      "Time: 0.015823841094970703\n",
      "\n",
      "Bounds:\n",
      "4 <= x1 <= 21\n",
      "-13 <= x2 <= 13\n",
      "Method: Powell\n",
      "Result: [ 4.00000000e+00 -1.98838969e-03]\n",
      "Memory: 8189\n",
      "Time: 0.020050048828125\n",
      "\n",
      "-------------------------\n",
      "Changing x1's bound\n",
      "Bounds:\n",
      "-11 <= x1 <= 21\n",
      "5 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [2.02461877e-08 5.00000003e+00]\n",
      "Memory: 26124\n",
      "Time: 0.12454557418823242\n",
      "\n",
      "Bounds:\n",
      "-10 <= x1 <= 21\n",
      "5 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [-1.67376724e-08  5.00000003e+00]\n",
      "Memory: 25555\n",
      "Time: 0.12929272651672363\n",
      "\n",
      "Bounds:\n",
      "-9 <= x1 <= 21\n",
      "5 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [-3.93607531e-09  5.00000001e+00]\n",
      "Memory: 26193\n",
      "Time: 0.12793946266174316\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 20\n",
      "5 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [2.05418640e-08 5.00000003e+00]\n",
      "Memory: 26960\n",
      "Time: 0.11487913131713867\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 19\n",
      "5 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [-8.12779611e-09  5.00000001e+00]\n",
      "Memory: 26228\n",
      "Time: 0.12406039237976074\n",
      "\n",
      "Changing x2's bound\n",
      "Bounds:\n",
      "-11 <= x1 <= 21\n",
      "6 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [-3.12294005e-08  6.00000002e+00]\n",
      "Memory: 27069\n",
      "Time: 0.11001324653625488\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 21\n",
      "7 <= x2 <= 13\n",
      "Method: trust-constr\n",
      "Result: [2.24653002e-09 7.00000002e+00]\n",
      "Memory: 26925\n",
      "Time: 0.10600399971008301\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 21\n",
      "5 <= x2 <= 12\n",
      "Method: trust-constr\n",
      "Result: [1.24408129e-08 5.00000003e+00]\n",
      "Memory: 26978\n",
      "Time: 0.1102294921875\n",
      "\n",
      "Bounds:\n",
      "-11 <= x1 <= 21\n",
      "5 <= x2 <= 11\n",
      "Method: trust-constr\n",
      "Result: [7.82795490e-08 5.00000003e+00]\n",
      "Memory: 27176\n",
      "Time: 0.12398386001586914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "\n",
    "for bound_method in BoundMethods:\n",
    "    test_scipy_bounds(f, bound_method.name.replace(\"BoundMethods\", \"\"), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:41.951310500Z",
     "start_time": "2023-06-21T14:07:40.607724100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "excel_saver.create_excel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-21T14:07:42.295881200Z",
     "start_time": "2023-06-21T14:07:41.956515400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
